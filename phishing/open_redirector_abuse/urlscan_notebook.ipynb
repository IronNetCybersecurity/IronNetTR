{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atlantic-candidate",
   "metadata": {},
   "source": [
    "# Notebook for retrieving final destination phishing domains from urlscan.io \n",
    "\n",
    "## Requirements\n",
    "* Python Libraries\n",
    "  * Pandas\n",
    "  * Requests\n",
    "* urlscan.io API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "from requests import get\n",
    "from getpass import getpass\n",
    "from urllib.parse import quote\n",
    "from json import loads\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-gospel",
   "metadata": {},
   "source": [
    "## Get urlscan.io data\n",
    "\n",
    "### REGEXs\n",
    "We have 2 REGEX queries that will find all the relevant final destination (FD) phishing domains with high accuracy. There are a few FPs that do come up however, we can filter those results out in later cell blocks. While we could combine these queries together with a optional capture group, i.e. `(\\-)?`, we are rate limited by 10000 results for each query by urlscan.io. \n",
    "\n",
    "#### REGEX 1: All FD domains with `-`\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\/$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\//\n",
    "```\n",
    "\n",
    "#### REGEX 2: All FD domains without `-`\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\/$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\//\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-agent",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "Using python requests can be slow, so sometimes using curl is better for grabbing a ton of results if you are in a time crunch.\n",
    "The first code block uses Python's requests library while the next code block loads from a file.\n",
    "\n",
    "#### Python Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-turkish",
   "metadata": {},
   "source": [
    "**Enter API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max results here!\n",
    "max_results = 100\n",
    "base_url = f'https://urlscan.io/api/v1/search/?size={max_results}&q='\n",
    "regex_one_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\//'\n",
    ")\n",
    "regex_two_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\//'\n",
    ")\n",
    "headers = {'api-key': api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_one_results = get(f'{base_url}{regex_one_query}', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_two_results = get(f'{base_url}{regex_two_query}', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-preliminary",
   "metadata": {},
   "source": [
    "#### Parse results from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse results from json file\n",
    "regex_one_file = ''\n",
    "regex_two_file = ''\n",
    "\n",
    "with open(regex_one_file, 'r') as file:\n",
    "    regex_one_results = loads(file.read())\n",
    "\n",
    "with open(regex_two_file, 'r') as file:\n",
    "    regex_two_results = loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'REGEX one results: {regex_one_results[\"total\"]}')\n",
    "print(f'REGEX two results: {regex_two_results[\"total\"]}')\n",
    "\n",
    "# Combine data\n",
    "regex_results = regex_one_results['results'] + regex_two_results['results']\n",
    "total_results_prefilter = len(regex_results)\n",
    "print(f'Total results: {total_results_prefilter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-bulletin",
   "metadata": {},
   "source": [
    "## Filter False Positives\n",
    "* Where path is `/Main/` and not `/main/`\n",
    "* Where task url is `/main/`. While this could remove some TPs, the FDs will likely pop up in other results.\n",
    "* Where task url's path is root\n",
    "* Remove google redirects\n",
    "* Where task url path does at least 1 `?` or `@` or `==` or `#` or `&`\n",
    "* Where domains are on the alexa 1 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexa 1 Million (domain line by line)\n",
    "alexa_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(alexa_file, 'r') as alexa:\n",
    "    alexa = alexa.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_regex_results = regex_results.copy()\n",
    "for i in regex_results:\n",
    "    if '.'.join(i['page']['domain'].split('.')[-2:]) in alexa or i['page']['url'][-6:] == '/Main/' or i['task']['url'][-6:] == '/main/' or '/goo.gl/' in i['task']['url'] or 'google.com/' in i['task']['url'] or ('&' not in i['task']['url'][8:] and '?' not in i['task']['url'] and '@' not in i['task']['url'] and '==' not in i['task']['url'] and '#' not in i['task']['url']):\n",
    "        x = 0\n",
    "        while x < len(filtered_regex_results):\n",
    "            if filtered_regex_results[x]['_id'] == i['_id']:\n",
    "                del filtered_regex_results[x]\n",
    "                break\n",
    "            x += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Results removed: {total_results_prefilter - len(filtered_regex_results)}')\n",
    "print(f'New total: {len(filtered_regex_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-estate",
   "metadata": {},
   "source": [
    "## Output Final Destination Phishing Domains\n",
    "\n",
    "**Enter output file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'phish_domains.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup domains\n",
    "domains = set()\n",
    "for i in filtered_regex_results:\n",
    "    domains.add(i['page']['domain'])\n",
    "\n",
    "# Write to file\n",
    "with open(output_file, 'w+') as file:\n",
    "    for domain in domains:\n",
    "        file.write(f\"{domain}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-block",
   "metadata": {},
   "source": [
    "**Optional: output phish domains with datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'phish_domains_with_datetime.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_and_page_urls = []\n",
    "for i in filtered_regex_results:\n",
    "    tasks_and_page_urls.append([i['task']['time'], i['page']['domain']])\n",
    "df = pd.DataFrame(tasks_and_page_urls, columns=['Task Time', 'Domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w+') as file:\n",
    "    file.write(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-drove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
