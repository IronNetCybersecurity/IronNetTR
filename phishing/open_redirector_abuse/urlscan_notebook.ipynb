{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hydraulic-motor",
   "metadata": {},
   "source": [
    "# Notebook for retrieving final destination phishing domains from urlscan.io \n",
    "\n",
    "## Requirements\n",
    "* Python Libraries\n",
    "  * Pandas\n",
    "  * Requests\n",
    "* urlscan.io API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "from requests import get\n",
    "from getpass import getpass\n",
    "from urllib.parse import quote\n",
    "from json import loads\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-paradise",
   "metadata": {},
   "source": [
    "## Get urlscan.io data\n",
    "\n",
    "### REGEXs\n",
    "We have 4 REGEX queries that will find all the relevant final destination (FD) phishing domains with high accuracy. There are a few FPs that do come up with the first 2 queries however, we can filter those results out in later cell blocks.\n",
    "\n",
    "#### REGEX 1: All FD domains with `-`\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\/$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\//\n",
    "```\n",
    "\n",
    "\n",
    "#### REGEX 2: All FD domains without `-`\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\/$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\//\n",
    "```\n",
    "\n",
    "\n",
    "#### REGEX 3: All FD URLs with `/jump/` path\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9\\-]+\\.((shop|online|club|com|top|si|org|net|live)|(xyz))\\/(?(6)(jump|main)|main)\\/$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9\\-]+\\.xyz\\/jump\\//\n",
    "```\n",
    "\n",
    "\n",
    "#### REGEX 4: All FD URLs with `.well-known` path\n",
    "**Base REGEX**\n",
    "\n",
    "```\n",
    "^(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z0-9\\-])+\\.([a-z])+\\/(.*)?\\.well-known\\/((login\\.php\\?ss=2(&)?.*)|(.*\\/authorize_client_id\\:.*))+$\n",
    "```\n",
    "\n",
    "**urlscan.io Query**\n",
    "```\n",
    "page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z0-9\\-])+\\.([a-z])+\\/(.*)?\\.well-known\\/((login\\.php\\?ss=2(&)?.*)|(.*\\/authorize_client_id\\:.*))+/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-business",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "Using python requests can sometimes be slow, so if you are using curl, you can also load results from a file.\n",
    "The first code block uses Python's requests library while the next code block loads from a file.\n",
    "\n",
    "#### Python Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-reform",
   "metadata": {},
   "source": [
    "**Enter API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max results here!\n",
    "max_results = 10000\n",
    "base_url = f'https://urlscan.io/api/v1/search/?size={max_results}&q='\n",
    "regex_one_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9]{1,}\\-[a-z0-9]{1,}\\.(shop|online|xyz|club|com|top|si|org|net|live)\\/main\\//'\n",
    ")\n",
    "regex_two_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z]|[0-9])+\\.(online|xyz|com|top|club|com.au|org.ge|co.ug|co.mz|one|org|net|live)\\/main\\//'\n",
    ")\n",
    "regex_three_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?[a-z0-9\\-]+\\.xyz\\/jump\\//'\n",
    ")\n",
    "regex_four_query = quote(\n",
    "    'page.url.keyword:/(http)(s)?:\\/\\/([a-z0-9]+\\.)?([a-z0-9\\-])+\\.([a-z])+\\/(.*)?\\.well-known\\/((login\\.php\\?ss=2(&)?.*)|(.*\\/authorize_client_id\\:.*))+/'\n",
    ")\n",
    "headers = {'api-key': api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_one_results = get(f'{base_url}{regex_one_query}', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_two_results = get(f'{base_url}{regex_two_query}', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_three_results = get(f'{base_url}{regex_three_query}', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_four_results = get(f'{base_url}{regex_four_query}', headers=headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-guess",
   "metadata": {},
   "source": [
    "#### Parse results from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse results from json file\n",
    "regex_one_file = ''\n",
    "regex_two_file = ''\n",
    "regex_three_file = ''\n",
    "regex_four_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(regex_one_file, 'r') as file:\n",
    "    regex_one_results = loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(regex_two_file, 'r') as file:\n",
    "    regex_two_results = loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(regex_three_file, 'r') as file:\n",
    "    regex_three_results = loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(regex_four_file, 'r') as file:\n",
    "    regex_four_results = loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'REGEX one results: {regex_one_results[\"total\"]}')\n",
    "print(f'REGEX two results: {regex_two_results[\"total\"]}')\n",
    "print(f'REGEX three results: {regex_three_results[\"total\"]}')\n",
    "print(f'REGEX four results: {regex_four_results[\"total\"]}')\n",
    "\n",
    "# Combine data\n",
    "regex_results = regex_one_results['results'] + regex_two_results['results'] + regex_three_results['results'] + regex_four_results['results']\n",
    "total_results_prefilter = len(regex_results)\n",
    "print(f'Total results: {total_results_prefilter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-flavor",
   "metadata": {},
   "source": [
    "## Filter False Positives\n",
    "* Where path is `/Main/` and not `/main/`\n",
    "* Where task url is `/main/`. While this could remove some TPs, the FDs will likely pop up in other results.\n",
    "* Where task url's path is root\n",
    "* Remove google redirects\n",
    "* Where task url path does at least 1 `?` or `@` or `==` or `#` or `&`\n",
    "* Where domains are on the alexa 1 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_regex_results = regex_results.copy()\n",
    "filtered_domains = set()\n",
    "for i in regex_results:\n",
    "    if i['page']['url'][-6:] == '/Main/' or i['task']['url'][-6:] == '/main/' or '/goo.gl/' in i['task']['url'] or 'google.com/' in i['task']['url'] or ('&' not in i['task']['url'][8:] and '?' not in i['task']['url'] and '@' not in i['task']['url'] and '=' not in i['task']['url'] and '#' not in i['task']['url']):\n",
    "        x = 0\n",
    "        while x < len(filtered_regex_results):\n",
    "            if filtered_regex_results[x]['_id'] == i['_id']:\n",
    "                filtered_domains.add(filtered_regex_results[x]['page']['domain'])\n",
    "                del filtered_regex_results[x]\n",
    "                break\n",
    "            x += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexa 1 Million (domain line by line)\n",
    "alexa_file = ''\n",
    "with open(alexa_file, 'r') as alexa:\n",
    "    alexa = alexa.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_alexa_domains = []\n",
    "for i in regex_results:\n",
    "    if '.'.join(i['page']['domain'].split('.')[-2:]) in alexa:\n",
    "        x = 0\n",
    "        while x < len(filtered_regex_results):\n",
    "            if filtered_regex_results[x]['_id'] == i['_id']:\n",
    "                filtered_alexa_domains.append(filtered_regex_results[x])\n",
    "                del filtered_regex_results[x]\n",
    "                break\n",
    "            x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domains filtered - These might be true positives but likely compromised sites.\n",
    "# If you would like to add these IOCs run this block.\n",
    "\n",
    "# Set threshold for alexa to be considered compromised\n",
    "threshold = 50000\n",
    "\n",
    "for i in filtered_alexa_domains:\n",
    "    x = 0\n",
    "    while x < len(alexa):\n",
    "        if '.'.join(i['page']['domain'].split('.')[-2:]) == alexa[x]:\n",
    "            if x >= threshold:\n",
    "                print(f\"Threshold met for {i['page']['domain']}: {x}\")\n",
    "                filtered_regex_results.append(i)\n",
    "            else:\n",
    "                print(f\"Threshold not met for {i['page']['domain']}: {x}\")\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Results removed: {total_results_prefilter - len(filtered_regex_results)}')\n",
    "print(f'New total: {len(filtered_regex_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-cattle",
   "metadata": {},
   "source": [
    "## Output Final Destination Phishing Domains\n",
    "\n",
    "**Enter output file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'phish_domains.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup domains\n",
    "domains = set()\n",
    "for i in filtered_regex_results:\n",
    "    domains.add(i['page']['domain'])\n",
    "\n",
    "print(f'Total phishing domains: {len(domains)}')\n",
    "\n",
    "# Write to file\n",
    "with open(output_file, 'w+') as file:\n",
    "    for domain in domains:\n",
    "        file.write(f\"{domain}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-dollar",
   "metadata": {},
   "source": [
    "**Optional: output phish domains with datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'phish_domains_with_datetime.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_and_page_urls = []\n",
    "for i in filtered_regex_results:\n",
    "    tasks_and_page_urls.append([i['task']['time'], i['page']['domain']])\n",
    "df = pd.DataFrame(tasks_and_page_urls, columns=['Task Time', 'Domain'])\n",
    "\n",
    "print(f'Total results: {len(tasks_and_page_urls)}')\n",
    "\n",
    "# Write to file\n",
    "with open(output_file, 'w+') as file:\n",
    "    file.write(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-acoustic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
